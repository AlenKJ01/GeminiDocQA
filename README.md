# GeminiDocQA
## ğŸ§  AI Chatbot Using Gemini 2.5 Pro

An intelligent, document-aware chatbot web application built with **FastAPI**, **Gemini 2.5 Pro**, **Hugging Face Transformers**, and **Sentence Transformers**.  
The chatbot can **summarize uploaded documents** and **answer user queries** based on document context â€” all via a clean HTML + CSS frontend, with no JavaScript.

---

## ğŸš€ Overview

This project demonstrates how to integrate **Googleâ€™s Gemini 2.5 Pro LLM** with a local vector-based retrieval pipeline.  
Users can upload **PDF**, **DOCX**, or **TXT** files, have them summarized automatically, and then **converse intelligently** with the document content.

It combines:
- NLP summarization (Hugging Face BART)
- Semantic embeddings (Sentence Transformers)
- Vector similarity search (pure NumPy cosine similarity)
- Conversational reasoning (Gemini 2.5 Pro)
- A minimal, responsive **FastAPI** web UI (HTML + CSS only)

---

## ğŸ§© Features

| Feature | Description |
|----------|-------------|
| ğŸ“„ **Document Upload** | Upload PDF, DOCX, or TXT files via FastAPI. |
| âœ‚ï¸ **Text Extraction** | Extracts text with `pdfplumber` or `python-docx`. |
| ğŸ§  **Summarization** | Uses `facebook/bart-large-cnn` from Hugging Face to produce concise summaries. |
| ğŸ§­ **Embeddings + Vector Store** | Splits text into chunks, embeds them using `all-MiniLM-L6-v2`, and stores vectors as `.npy` arrays (no FAISS dependency). |
| ğŸ’¬ **Chat with Document** | Ask contextual questions â€” Gemini 2.5 Pro retrieves relevant chunks and answers only from your uploaded document. |
| ğŸ’¾ **Session Management** | Keeps conversation history for contextual responses. |
| ğŸ¨ **Frontend** | Single-page HTML + CSS interface (no JavaScript). |
| âš™ï¸ **Robust Error Handling** | Handles large files, missing tokens, summarization retries, and model timeouts gracefully. |

---

## ğŸ—ï¸ Project Structure
```
AI-Chatbot-using-Gemini/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â”œâ”€â”€ utils.py # Core NLP, embeddings, and Gemini logic
â”‚ â”œâ”€â”€ templates/
â”‚ â”‚ â””â”€â”€ index.html # Single-page web UI
â”‚ â””â”€â”€ static/
â”‚ â””â”€â”€ style.css # Minimal responsive styling
â”‚
â”œâ”€â”€ .env # API keys and environment variables
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Project documentation
```

---

## âš™ï¸ Tech Stack

| Layer | Tools / Libraries |
|-------|--------------------|
| **Frontend** | HTML, CSS (no JS) |
| **Backend** | FastAPI, Uvicorn |
| **Text Extraction** | pdfplumber, python-docx |
| **Summarization** | Hugging Face Transformers (`facebook/bart-large-cnn`) |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) |
| **Vector Search** | NumPy cosine similarity |
| **LLM** | Gemini 2.5 Pro (`google-generativeai`) |
| **Env Management** | python-dotenv |

---

## ğŸ”‘ Environment Variables (`.env`)

Create a `.env` file in the project root:

```env
# Google Gemini API key
GEMINI_API_KEY=your_gemini_api_key_here

# Hugging Face token for model downloads
HF_TOKEN=your_huggingface_token_here

# Optional custom model name
GEMINI_MODEL_NAME=gemini-2.5-pro
```

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/AI-Chatbot-using-Gemini.git
cd AI-Chatbot-using-Gemini
```

### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv venv
venv\Scripts\activate     # Windows
# or
source venv/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

If you donâ€™t have a requirements.txt, use:

pip install fastapi uvicorn jinja2 python-multipart pdfplumber python-docx transformers torch sentence-transformers langchain google-generativeai python-dotenv typing_extensions

### 4ï¸âƒ£ Set up environment variables

Add your .env file (as shown above) with valid API keys.

### 5ï¸âƒ£ Run the app
```bash
uvicorn app.main:app --port 8000
```

Visit ğŸ‘‰ 
## http://127.0.0.1:8000

---

## ğŸ§® Workflow

1. Upload Document
  - Choose a .pdf, .docx, or .txt file.
  - FastAPI extracts its text and displays it.

2. Generate Summary
  - Summarization is performed using the BART transformer model.
  - Large texts are chunked, summarized per part, and combined.

3. Create Vectorstore
  - Text is divided into overlapping chunks.
  - Embeddings generated via all-MiniLM-L6-v2.
  - Stored locally in .npy for semantic retrieval.

4. Chat with Document
  - User question â†’ semantic search retrieves relevant chunks â†’ Gemini 2.5 Pro forms the answer using only that context.
  - The conversation remains contextual for multiple queries.

---

## ğŸ§  Example Flow

1. Upload a project report PDF.

2. Click Generate Summary â€” see concise summary of all sections.

3. Click Create Vectorstore.

4. Ask questions like:
  - "What are the key results discussed in the report?"
  - "Which technologies were used?"
  - "Summarize the conclusions."

---
## ğŸ› ï¸ Troubleshooting

| Problem                                        | Solution                                                               |
| ---------------------------------------------- | ---------------------------------------------------------------------- |
| **`Unauthorized 401` when loading BART model** | Add your `HF_TOKEN` in `.env`                                          |
| **`ModuleNotFoundError: faiss`**               | FAISS removed â€” ensure `utils.py` version uses NumPy search            |
| **Gemini returns empty / `MAX_TOKENS`**        | New parser retries automatically; adjust `max_output_tokens` if needed |
| **Windows crash on shutdown**                  | Use `asyncio.WindowsSelectorEventLoopPolicy()` (already applied)       |
